{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select kernal Python 3.12.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1: NASA APOD Data Retrieval and JSON File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv('API_KEY')\n",
    "api = os.getenv('API_URL')\n",
    "# To ensure connection.\n",
    "print(f\"API URL is: {api}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APICall\n",
    "import requests\n",
    "\n",
    "def get_apod_data(api_key, date):\n",
    "    url  = f\"{api}/planetary/apod?api_key={api_key}&date={date}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        #make sure status is 200\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return {\n",
    "            \"date\": data[\"date\"],\n",
    "            \"title\": data[\"title\"],\n",
    "            \"url\": data[\"url\"],\n",
    "            \"explanation\": data[\"explanation\"],\n",
    "            \"media_type\": data[\"media_type\"]\n",
    "\n",
    "        }\n",
    "    \n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(\"got error at RequestException\")\n",
    "        print(error)\n",
    "    except KeyError:\n",
    "        print(\"Invalid response\")\n",
    "\n",
    "date = get_apod_data(key , \"2020-01-01\")\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Data for a DateRange\n",
    "import time\n",
    "from datetime import datetime , timedelta\n",
    "\n",
    "def fetch_multiple_apod_data(api_key, start_date, end_date):\n",
    "   start_date = datetime.strptime(start_date , \"%Y-%m-%d\")\n",
    "   end_date = datetime.strptime(end_date , \"%Y-%m-%d\")\n",
    "\n",
    "   wholeData = []\n",
    "   in_loop_date = start_date\n",
    "   \n",
    "   while in_loop_date <= end_date:\n",
    "       formatted_date = in_loop_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "       date_data = get_apod_data(api_key ,formatted_date)\n",
    "       \n",
    "       wholeData.append(date_data)\n",
    "       in_loop_date += timedelta(days=1)\n",
    "       time.sleep(1)\n",
    "       \n",
    "       \n",
    "   return wholeData\n",
    "\n",
    "start_date = \"2021-01-01\"\n",
    "end_date = \"2021-12-31\" \n",
    "year_data = fetch_multiple_apod_data(key, start_date, end_date)\n",
    "\n",
    "\n",
    "file_name = \"retrieved_data.json\"\n",
    "if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w') as file:\n",
    "            json.dump([], file) \n",
    "\n",
    "# Read the existing data from the file.\n",
    "with open(file_name, 'r') as file:\n",
    "    wholeData = json.load(file)\n",
    "   \n",
    " # Write the updated data back to the file.\n",
    "with open(file_name, 'w') as file:\n",
    "    json.dump(year_data, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: JSON Data Reading,Looping,and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_apod_data():\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(\"retrieved_data.json\", \"r\") as file:\n",
    "            apod_data = json.load(file)  # Load JSON into a dictionary\n",
    "            return apod_data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'apod_data.json' file not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: 'apod_data.json' contains invalid JSON.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "data = read_apod_data()\n",
    "\n",
    "for record in data:\n",
    "  \n",
    "    print({\n",
    "            \"date\": record[\"date\"],\n",
    "            \"title\": record[\"title\"]\n",
    "         }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_apod_media(data):\n",
    "    totalImage = 0\n",
    "    totalVideo = 0\n",
    "\n",
    "    # Loop through each record and count media types\n",
    "    for record in data:\n",
    "        if record[\"media_type\"] == \"image\":\n",
    "            totalImage += 1\n",
    "        elif record[\"media_type\"] == \"video\":\n",
    "            totalVideo += 1\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Total count of Images: {totalImage}\")\n",
    "    print(f\"Total count of Videos: {totalVideo}\")\n",
    "    \n",
    "    # Find the entry with the longest explanation\n",
    "    longest_text = max(data, key=lambda entry: len(entry.get(\"explanation\", \"\")))\n",
    "    print(\"Entry with the longest explanation:\")\n",
    "    print(json.dumps(longest_text, indent=4))\n",
    "\n",
    "analyze_apod_media(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def write_apod_to_csv(json_file, csv_file):\n",
    "    try:\n",
    "        # Load the JSON file\n",
    "        with open(json_file, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Open the CSV file in append mode (create it if it doesn't exist)\n",
    "        file_exists = os.path.exists(csv_file)\n",
    "        \n",
    "        with open(csv_file, mode='a', newline='') as csvfile:\n",
    "            fieldnames = ['date', 'title', 'media_type', 'url']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write headers only if the file doesn't already exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            # Loop through the data and write each entry to the CSV\n",
    "            for entry in data:\n",
    "                writer.writerow({\n",
    "                    'date': entry['date'],\n",
    "                    'title': entry['title'],\n",
    "                    'media_type': entry['media_type'],\n",
    "                    'url': entry['url']\n",
    "                })\n",
    "        \n",
    "        print(f\"Data successfully written to {csv_file}.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file}' not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format in the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "write_apod_to_csv(\"retrieved_data.json\", \"apod_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem3- Numpy Array Manipulation and Statistical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrates random numbers between 10 to 100 for 20 rows and 5 columns and saves the output in var data.\n",
    "data = np.random.randint(10, 100, size=(20, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the dataset.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate the sum of each row. \n",
    "for d in range(data.shape[0]):\n",
    "    # If sum/2 has a reminder, subtract 1 from the first element in the row to get an even number.\n",
    "    rowSum= np.sum(data[d])\n",
    "    if rowSum % 2 !=0:\n",
    "        data[d,0] -=1\n",
    "    print(rowSum) \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total sum and the remainder.\n",
    "SumOfValues = np.sum(data)\n",
    "remaining = SumOfValues % 5\n",
    "\n",
    "if remaining == 0:\n",
    "    print(f\"Orginal Sum is a multiple of 5: {SumOfValues}\")\n",
    "else:\n",
    "    # Adjust the first element to make the total sum a multiple of 5.\n",
    "    if data[0, 0] >= remaining:\n",
    "        data[0, 0] -= remaining\n",
    "    else:\n",
    "        data[0, 0] += (5 - remaining)\n",
    "    print(f\"Updated Array:\\n{data}\")\n",
    "    print(f\"Updated Sum: {np.sum(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty array that will collect divisible digits.\n",
    "divisible = []\n",
    "for row in range(data.shape[0]):\n",
    "    for d in data[row]:\n",
    "        # Condtion must be meet for bith 3 and 5.\n",
    "        if d % 3 == 0 and d % 5 == 0:\n",
    "            # Adds  divisible digit to array divisible.\n",
    "           divisible.append(d) \n",
    "print(f\"Numbers divisible by 3 and 5 are:\\n {divisible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace elements greater than 75 with the mean value\n",
    "data[data > 75] = np.mean(data)\n",
    "\n",
    "print(f\"Edited Array:\\n {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of all values in the array.\n",
    "mean_value = np.mean(data)\n",
    "print(f\"\\nMean of the array: {mean_value}\")\n",
    "\n",
    "# Standard deviation of all values in the array\n",
    "std_dev = np.std(data)\n",
    "print(f\"Standard Deviation of the array: {std_dev}\")\n",
    "\n",
    "# Median of all values in the array\n",
    "median_value = np.median(data)\n",
    "print(f\"Median of the array: {median_value}\")\n",
    "\n",
    "# Variance of each column\n",
    "variance_columns = np.var(data, axis=0)\n",
    "print(f\"Variance for each column: {variance_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4 -Working with Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read iris.csv into Python as a pandas.\n",
    "irisData= pd.read_csv('iris.csv')\n",
    "irisData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size.\n",
    "irisData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types.\n",
    "irisData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all the columns.\n",
    "irisData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species of flower.\n",
    "irisData.Species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  List of affected rows.\n",
    "affectedRows = irisData.iloc[[35, 38]]\n",
    "affectedRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating rows.\n",
    "irisData.iloc[35] = [4.9, 3.1, 1.5, 0.2, \"setosa\"]  \n",
    "irisData.iloc[38] = [4.9, 3.6, 1.4, 0.1, \"setosa\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
      "35           4.9          3.1           1.5          0.2  setosa\n",
      "38           4.9          3.6           1.4          0.1  setosa\n"
     ]
    }
   ],
   "source": [
    "# Checking fix.\n",
    "updatedRows = irisData.iloc[[35,38]] \n",
    "print(updatedRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features, which are the ratio of 2 columns.\n",
    "irisData['Petal.Ratio'] = irisData['Petal.Length']/irisData['Petal.Width']\n",
    "irisData['Sepal.Ratio'] = irisData['Sepal.Length']/irisData['Sepal.Width']\n",
    "irisData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to cvs as iris_corrected.csv.\n",
    "irisData.to_csv(\"iris_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise correlation.\n",
    "# Remove categorical feature Species.\n",
    "numericColumns = irisData.drop('Species', axis=1)\n",
    "numericColumns\n",
    "correlation_matrix = numericColumns.corr()  \n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot with Sepal Ratio on the x-axis and Petal Ratio on the y-axis\n",
    "from scipy.stats import linregress\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(irisData['Sepal.Ratio'], irisData['Petal.Ratio'], color='green')\n",
    "plt.title('Sepal.Ratio vs Petal.Ratio')\n",
    "plt.xlabel('Sepal.Ratio')\n",
    "plt.ylabel('Petal.Ratio %')\n",
    "sns.regplot(data=irisData, x='Sepal.Ratio', y='Petal.Ratio')\n",
    "plt.savefig(\"iris_scatter_with_regression.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a pair plot of all numericColumns\n",
    "sns.pairplot(numericColumns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1eab06d1d2455bf31171241161ade224baca4f101ec29ecd1e7fd7c9a928477"
  },
  "kernelspec": {
   "display_name": "Python 3.12.7 ('pr_ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
